<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title>Image processing</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (https://tug.org/tex4ht/)"> 
<meta name="originator" content="TeX4ht (https://tug.org/tex4ht/)"> 
<!-- html --> 
<meta name="src" content="m_image_processing.tex"> 
<link rel="stylesheet" type="text/css" href="m_image_processing.css"> 
</head><body 
>
   <div class="maketitle">
                                                                                    
                                                                                    
                                                                                    
                                                                                    
                                                                                    
                                                                                    

<h2 class="titleHead">Image processing</h2>
 <div class="author" ><a 
href="https://github.com/Grufoony/Physics_Unibo" class="url" ><span 
class="cmtt-12x-x-120">https://github.com/Grufoony/Physics_Unibo</span></a></div><br />
<div class="date" ></div>
                                                                                    
                                                                                    
   </div>
                                                                                    
                                                                                    
   <h2 class="likechapterHead"><a 
 id="x1-1000"></a>Contents</h2>
   <div class="tableofcontents">
   <span class="chapterToc" >1 <a 
href="#x1-20001" id="QQ2-1-2">Introduction to images</a></span>
<br />   <span class="chapterToc" >2 <a 
href="#x1-30002" id="QQ2-1-3">Digital images</a></span>
<br />   <span class="sectionToc" >2.1 <a 
href="#x1-40002.1" id="QQ2-1-4">Radiometry</a></span>
<br />   <span class="sectionToc" >2.2 <a 
href="#x1-50002.2" id="QQ2-1-5">Production of images</a></span>
<br />   <span class="sectionToc" >2.3 <a 
href="#x1-60002.3" id="QQ2-1-6">Quality of images</a></span>
<br />   <span class="chapterToc" >3 <a 
href="#x1-70003" id="QQ2-1-7">Digital image processing</a></span>
   </div>
                                                                                    
                                                                                    
                                                                                    
                                                                                    
   <h2 class="chapterHead"><span class="titlemark">Chapter&#x00A0;1</span><br /><a 
 id="x1-20001"></a>Introduction to images</h2>
<!--l. 1--><p class="noindent" >The first question that someone should ask themselves is, of course, what is an image?
<br 
class="newline" />We know from everyday life that we can&#8217;t have an image without light. In more general
terms, in order to have light we need to have some sort of radiation. <br 
class="newline" />So we can say that an image is a distribution of matter, which is called an <span 
class="cmti-12">object</span>, that is
visible when illuminated. Alternatively it can be defined as a measure of the intensity of the
reflected radiation. <br 
class="newline" /><br 
class="newline" />Any imaging technique is characterized by the way that the object and the radiation interact.
An imaging system collects radiation emitted by objects. <br 
class="newline" /><br 
class="newline" />We define two quantities: The energy intensity, <span 
class="cmmi-12">E</span>, and the energy flux, <span 
class="cmmi-12">Q</span>. <br 
class="newline" />Through this two quantities we can define <span 
class="cmti-12">irradiance</span>, <span 
class="cmti-12">radial intensity </span>and <span 
class="cmti-12">radiance</span>:
   <div class="math-display" >
<img 
src="m_image_processing0x.svg" alt="                 [   ]
             dQ-  -W-
irradiance  = dA   m2
" class="math-display" ></div>
<!--l. 10--><p class="indent" >
   <div class="math-display" >
<img 
src="m_image_processing1x.svg" alt="                  dQ [   W   ]
radialintensity =  ---  -------
                  d&#x03C9;   sterad
" class="math-display" ></div>
<!--l. 13--><p class="indent" >
                                                                                    
                                                                                    
   <div class="math-display" >
<img 
src="m_image_processing2x.svg" alt="                  [            ]
            --dQ--  ----W------
radiance  = d&#x03C9;dA    sterac &#x22C5; m2
" class="math-display" ></div>
<!--l. 16--><p class="indent" >   Usually sources of radiation are polychromatic, so there is a spectrum. It is thus essential
to know how the radiation interacts with objects. <br 
class="newline" />Different wavelengths interact differently, thus producing different images. This can be used
in what is called <span 
class="cmti-12">multispectral imaging</span>.
                                                                                    
                                                                                    
   <h2 class="chapterHead"><span class="titlemark">Chapter&#x00A0;2</span><br /><a 
 id="x1-30002"></a>Digital images</h2>
<!--l. 1--><p class="noindent" >When talking about digital images, we must define the concepts of geometry and radiometry.
<br 
class="newline" />Geometry is the relationship between location and size of the objects in the 3D world and
their representation in the image plane. <br 
class="newline" />Radiometry is the relationship between the amount of light radiating from a point and the
amount of light impinging on the correspondend image point. <br 
class="newline" /><br 
class="newline" />The most basic image formation device (and the first one historically) is the pin-hole camera.
The pin-hole camera consists of a closed room with a small hole (of the order of the
millimiter) in one of the walls. When the light coming from the object reaches the hole,
the image is formed upside down in the opposite wall of the chamber. The size of
the image in the image plane depends on the object&#8217;s distance from the pin-hole.
<br 
class="newline" /><br 
class="newline" />If a point <span 
class="cmmi-12">M </span>in the 3D space is characterized by 3 coordinates (<span 
class="cmmi-12">x,y,z</span>), the image point in the
image plane is characterized by 2 coordinates (<span 
class="cmmi-12">u,v</span>). The two sets of coordinates are related
by the geometrical equations
   <div class="math-display" >
<img 
src="m_image_processing3x.svg" alt="     fx    f y
u =  ---   ---
     z      z
" class="math-display" ></div>
<!--l. 9--><p class="indent" >   All the light rays are considered to be parallel to the optical axis and orthogonal to the
image plane. <br 
class="newline" />We define &#x0394;<span 
class="cmmi-12">z </span>as the size of the object with respect to its distance from the camera, so as its
thickness. Then, if 2&#x0394;<span 
class="cmmi-12">z </span>is small with respect to <span 
class="cmmi-12">z</span><sub><span 
class="cmr-8">0</span></sub>, we have that
   <div class="math-display" >
<img 
src="m_image_processing4x.svg" alt="---f----   ---f----   -f
z0 + &#x0394;z &#x2248;  z0 - &#x0394;z  &#x2248; z0
" class="math-display" ></div>
                                                                                    
                                                                                    
<!--l. 14--><p class="indent" >   which means that
   <div class="math-display" >
<img 
src="m_image_processing5x.svg" alt="u &#x2248;  f-x    v &#x2248;  f-y
     z0          z0
" class="math-display" ></div>
<!--l. 18--><p class="indent" >   This approximation of course is only valid for small objects, or objects that are close to
the optical axis. <br 
class="newline" />The quality of the image depends heavily on the size of the hole.
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 21--><p class="noindent" >If the hole is too big, we have that a point of the object becomes a spot in the
      image. This means that the image is going to be blurry.
      </li>
      <li class="itemize">
      <!--l. 22--><p class="noindent" >If the hole is too small, we can have diffraction effects, that again, end up blurring
      the image.</li></ul>
<!--l. 24--><p class="noindent" >The solution for balancing the effects of big and small pinholes and settling for a middle ground
is in the use of lenses. <br 
class="newline" /><br 
class="newline" />Lenses have two main strengths: They allow to gather light coming from a point on the
object and focus it into a single point on the image; since the aperture size of the
lense is larger than that of a pinhole, the exposure times can be reduced as well.
<br 
class="newline" />Lenses solve another problem as well: In a pinhole camera, we have that many points from
the object space are mapped into a single in the image plane. So, an image on the image
plane can come from several objects in the real space. <br 
class="newline" />On the other hand, a lens brings into focus only those object points that lie within one
particular plane parallel to the image plane. <br 
class="newline" />So, when the distance between lens and image plane is equal to <span 
class="cmmi-12">v</span>, only those points that are
at a distance <span 
class="cmmi-12">u </span>are brought into focus, with <span 
class="cmmi-12">u </span>give by
                                                                                    
                                                                                    
   <div class="math-display" >
<img 
src="m_image_processing6x.svg" alt=" 1   1    1           vf
u-+  v-=  f-  &#x003E; u = v---f-
" class="math-display" ></div>
<!--l. 32--><p class="indent" >   In other words, if we bring an object into focus at a distance <span 
class="cmmi-12">u</span>, we must set the distance <span 
class="cmmi-12">v</span>
between the image plane and the lens to
   <div class="math-display" >
<img 
src="m_image_processing7x.svg" alt="v = --uf--
    u -  f
" class="math-display" ></div>
<!--l. 36--><p class="indent" >   The object points that do not lie within this plane end up being blurred. <br 
class="newline" /><br 
class="newline" />In a real camera ther is an object called diaphragm, whose purpose is to control the opening
of the lens. By reducing the size of the aperture, it reduces the amount of light that reaches
the sensor, and also reduces the size of the circle of confusion, thus increasing the depth of
field. <br 
class="newline" />The <span 
class="cmti-12">field of view </span>is defined as the portion of space that actually projects onto the camera. It
describes then the cone of viewing directions of the device. <br 
class="newline" />The FOV depends of the effective area of the image sensor, the width <span 
class="cmmi-12">w </span>and the height
<span 
class="cmmi-12">h</span>:
   <div class="math-display" >
<img 
src="m_image_processing8x.svg" alt="F OV   = 2arctan -w-  F OV   = 2 arctan -h-
     v           2f        h           2f
" class="math-display" ></div>
<!--l. 43--><p class="indent" >   The <span 
class="cmti-12">magnification factor </span>is then defined as
                                                                                    
                                                                                    
   <div class="math-display" >
<img 
src="m_image_processing9x.svg" alt="      x     v   f
M  =  X--=  u-= u-
" class="math-display" ></div>
<!--l. 47--><p class="indent" >   where <span 
class="cmmi-12">x </span>is the size of the image whereas <span 
class="cmmi-12">X </span>is the size of the real object. <br 
class="newline" />We see clearly that the magnification factor is proportional to the focal length.
<br 
class="newline" />Since the FOV depends on the focal length as well, we can say that the magnification factor
and the FOV are linked. In particular:
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 51--><p class="noindent" >if <span 
class="cmmi-12">f </span>is small -¿ large FOV, small M
      </li>
      <li class="itemize">
      <!--l. 52--><p class="noindent" >if <span 
class="cmmi-12">f </span>is large -¿ small FOV, large M</li></ul>
   <h3 class="sectionHead"><span class="titlemark">2.1   </span> <a 
 id="x1-40002.1"></a>Radiometry</h3>
<!--l. 1--><p class="noindent" >Radiometry enables us to know what a pixel value implies about surface lighness and
illumination. So radiometry links the effective brightness of an object point with the
respective image point&#8217;s pixel value. <br 
class="newline" /><br 
class="newline" />The amount of object coming out of an object, <span 
class="cmmi-12">f</span>(<span 
class="cmmi-12">x,y</span>), can be expressed as
   <div class="math-display" >
<img 
src="m_image_processing10x.svg" alt="f (x, y) = i(x, y)r(x,y)
" class="math-display" ></div>
<!--l. 6--><p class="indent" >   with 0 <span 
class="cmmi-12">&#x003C; f</span>(<span 
class="cmmi-12">x,y</span>) <span 
class="cmmi-12">&#x003C; </span><span 
class="cmsy-10x-x-120">&#x221E;</span>, 0 <span 
class="cmmi-12">&#x003C; i</span>(<span 
class="cmmi-12">x,y</span>) <span 
class="cmmi-12">&#x003C; </span><span 
class="cmsy-10x-x-120">&#x221E; </span>and 0 <span 
class="cmmi-12">&#x003C; r</span>(<span 
class="cmmi-12">x,y</span>) <span 
class="cmmi-12">&#x003C; </span>1, where <span 
class="cmmi-12">i</span>(<span 
class="cmmi-12">x,y</span>) is the light
coming from the source and <span 
class="cmmi-12">r</span>(<span 
class="cmmi-12">x,y</span>) is the object&#8217;s reflectance. <br 
class="newline" /><span 
class="cmmi-12">i</span>(<span 
class="cmmi-12">x,y</span>) is determined by the light source, whereas <span 
class="cmmi-12">r</span>(<span 
class="cmmi-12">x,y</span>) depends on the surface of the object.
<br 
class="newline" /><br 
class="newline" />An acquisition device can be describd as a system that, given an input <span 
class="cmmi-12">f</span>(<span 
class="cmmi-12">&#x03BE;,&#x03B7;</span>), produces an
output <span 
class="cmmi-12">g</span>(<span 
class="cmmi-12">x,y</span>), which represents the acquired image. <br 
class="newline" />The two functions are usually related by
   <div class="math-display" >
<img 
src="m_image_processing11x.svg" alt="          &#x222B; &#x221E;
g(x, y) =     h (x, y,&#x03BE;,&#x03B7;)f(&#x03BE;,&#x03B7;)d &#x03BE;d&#x03B7;
           -&#x221E;
" class="math-display" ></div>
<!--l. 13--><p class="indent" >   where <span 
class="cmmi-12">h</span>(<span 
class="cmmi-12">x,y,&#x03BE;,&#x03B7;</span>) is the system&#8217;s response when presented with the unit impulse as unit,
so it defines how a point (<span 
class="cmmi-12">&#x03BE;,&#x03B7;</span>) in the object space contributes to the formation of the image
in a particular point (<span 
class="cmmi-12">x,y</span>). <br 
class="newline" />In other words, <span 
class="cmmi-12">h </span>describes the distorsions introduced by the system. <br 
class="newline" />For linear and shift invariant processes, the previous relation can be simplified and written
as
   <div class="math-display" >
<img 
src="m_image_processing12x.svg" alt="          &#x222B; &#x221E;
g (x,y) =     h (x - &#x03BE;,y - &#x03B7;)f(&#x03BE;,&#x03B7;)d&#x03BE;d &#x03B7;
           -&#x221E;
" class="math-display" ></div>
<!--l. 19--><p class="indent" >   Acquiring a series of static images, one would expect that the intensity value <span 
class="cmmi-12">g </span>in one
point remains the same for all the images, but this doesn&#8217;t always happen, and we can see
fluctuations of the <span 
class="cmmi-12">g </span>value around a certain level. <br 
class="newline" />In this cases we say that noise is present and the value&#8217;s fluctuations are a way to measure its
value. <br 
class="newline" />Noise can be defined as the uncertainty or imprecision with which a signal is recorded.
<br 
class="newline" />Images can have many sources of noise, like variations in the source&#8217;s emission, electronic
noise, interferences and so on. To take this effects into account in the previous formula we
simply add a term representing the noise.
                                                                                    
                                                                                    
   <div class="math-display" >
<img 
src="m_image_processing13x.svg" alt="         &#x222B;
           &#x221E;
g(x,y) =      h(x - &#x03BE;,y - &#x03B7;)f(&#x03BE;,&#x03B7; )d&#x03BE;d&#x03B7; + n(x,y )
          - &#x221E;
" class="math-display" ></div>
<!--l. 26--><p class="indent" >   where in many situations we can model <span 
class="cmmi-12">n</span>(<span 
class="cmmi-12">x,y</span>) to be gaussian.
   <h3 class="sectionHead"><span class="titlemark">2.2   </span> <a 
 id="x1-50002.2"></a>Production of images</h3>
<!--l. 1--><p class="noindent" >Acquisition devices register the amount of radiation impinging each point of the system as
analog signals, and this produces digital images, which are basically numerical
representations of an object. This means that analog signals need to be converted, through a
process called digitization, and after that the digital images are made available for computer
processing. <br 
class="newline" /><br 
class="newline" />So the production of a digital image can be divided into three steps:
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 4--><p class="noindent" >Measure of the analog signal.
      </li>
      <li class="itemize">
      <!--l. 5--><p class="noindent" >Sampling, the process of measuring the grey levels for each pixel location.
      </li>
      <li class="itemize">
      <!--l. 6--><p class="noindent" >Quantization, the process of dividing the scale into a discrete set.</li></ul>
<!--l. 8--><p class="noindent" >So digitization, that convers an image from its original form into digital form, is the combination
of this three step, that convers an image from its original form into digital forms.
<br 
class="newline" /><br 
class="newline" />The number inserted into the digital image at each pixel location (which is the point&#8217;s grey
level) reflects the brightness of the image at the corresponding point, which as we have just
said is sampled and quantizied. <br 
class="newline" />This means that a digital image is basically a matrix, that for each element contains an
integer, corresponding to the pixel&#8217;s grey level. <br 
class="newline" />Digital images are characterized by two resolutions: The spatial resolution (sampling density)
                                                                                    
                                                                                    
and the grey-level resolution. The first is the pixel spacing, so the number of sample points
per unit of measure, whereas the grey-level resolution is the number of grey-levels.
<br 
class="newline" />Spatial sampling can be described as a multiplication of the function <span 
class="cmmi-12">f</span>(<span 
class="cmmi-12">x,y</span>) and the function
<span 
class="cmmi-12">s</span>(<span 
class="cmmi-12">x,y</span>), which is defined as
   <div class="math-display" >
<img 
src="m_image_processing14x.svg" alt="         &#x2211;   &#x2211;
s(x,y) =         &#x03B4;(x -  i&#x0394;x, y - j&#x0394;y )
           i  j
" class="math-display" ></div>
<!--l. 16--><p class="indent" >   that basically defines the sample grid. <br 
class="newline" />So the sample image can be defined as
   <div class="math-display" >
<img 
src="m_image_processing15x.svg" alt="                         &#x2211;   &#x2211;
fc(x, y) = s(x,y)f(x,y ) =        f(i&#x0394;x, j&#x0394;y )&#x03B4;(x - i&#x0394;x, y - j&#x0394;y )
                           i  j
" class="math-display" ></div>
<!--l. 21--><p class="indent" >   So what we do is discretize the input, and in the end the sampled image only consists of
the samples acquired at each node of the sampling grid. <br 
class="newline" /><br 
class="newline" />At this point we can ask ourselves, how many samples and grey levels do we need to
represent the object in an acceptable or good way? <br 
class="newline" />Intuitively, we can say that the pixel size should be of comparable size with with the smalles
detail that we want to perceive from the image. <br 
class="newline" />More quantitatively, the <span 
class="cmti-12">Nyquist rate </span>says that the sampling interval must not be greater
that half the size of the smallest resolvable feature of the image. <br 
class="newline" />If the sampling interval is too large, we can have aliasing. <br 
class="newline" />If the spatial resolution is too low, what we get is that the image becomes very pixellated, so
we aren&#8217;t able to distinguish small details anymore. On the other hand, if the grey-level
resolution is too low, we lose the colour difference of neighbouring points, and this leads again
to a loss of detail. <br 
class="newline" /><br 
class="newline" />When choosing the resolution, we must balance pros and cons to find the right middle
ground. <br 
class="newline" />This is because, while it is true that high resolution images contain more information, all that
information might not be needed, but it would still require more storage space and execution
time for the various acquisition and processing steps. Furthermore, high definition images
require all the acquisition, processing and visualization devices to support that level of
definition. And finally, low risolution images are less affected by statistical noise.
<br 
class="newline" /><br 
class="newline" />A digital image can be described as a matrix
   <div class="math-display" >
<img 
src="m_image_processing16x.svg" alt="         (                                                )
              f(0,0)       f (0,1)    ...    f(0,N  - 1)
         |    f(1,0)       f (1,1)    ...    f(1,N  - 1)  |
f(x,y ) = ||     ...            ...      ...        ...      ||
         |(                                                |)
           f (M  - 1,0)  f(M  - 1,1)  ... f(M  - 1, N - 1 )
" class="math-display" ></div>
<!--l. 39--><p class="indent" >   or also as a vector.
<!--l. 61--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">2.3   </span> <a 
 id="x1-60002.3"></a>Quality of images</h3>
<!--l. 1--><p class="noindent" >When we apply a certain operation to an image, we are obviously trying to improve its
quality. In order to be able to say if we have managed to do so, we need a criteria for
assessing the quality of an image. <br 
class="newline" />There are a lot of sources for image degradation, so it is important to find a way to quantify
image quality. The quality assessment can be subjective or objective:
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 4--><p class="noindent" >subjective: involving human observers. The best way to find the quality of an
      image is to look at it, because the human eye is going to be the final observer of
      any image.
                                                                                    
                                                                                    
      </li>
      <li class="itemize">
      <!--l. 5--><p class="noindent" >objective: the goal of objective evaluation is to develop a quantitative measure
      that can assess the distortions in the images. A possible way to evaluate objective
      image quality is through the use of the mean squared error
<div class="math-display" >
<img 
src="m_image_processing17x.svg" alt="     &#x2211;  (g (n ) - g (n)2)
E =  --n-&#x2211;i------------
           n gi(n )2
" class="math-display" ></div>
      </li></ul>
<!--l. 10--><p class="noindent" >where <span 
class="cmmi-12">g</span><sub><span 
class="cmmi-8">i</span></sub> represents the ideal image and <span 
class="cmmi-12">g </span>represents the reconstructed image.
                                                                                    
                                                                                    
<!--l. 63--><p class="indent" >
                                                                                    
                                                                                    
   <h2 class="chapterHead"><span class="titlemark">Chapter&#x00A0;3</span><br /><a 
 id="x1-70003"></a>Digital image processing</h2>
<!--l. 1--><p class="noindent" >Geometric transformations are common in computer graphics, and are often used in
image analysis. They basically consist of rearranging pixels in the image plane.
<br 
class="newline" />A geometric transform consists of two basic steps:
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 4--><p class="noindent" >determining the pixel coordinate transformation mapping of the coordinates of
      the moving image pixel to the point in the fixed image.
      </li>
      <li class="itemize">
      <!--l. 5--><p class="noindent" >determining the brightness of the points in the digital grid of the transformed
      image.</li></ul>
<!--l. 7--><p class="noindent" >The most common geometric transformations are rotations, reflections, translations and scaling
(shrink or zoom). <br 
class="newline" /><br 
class="newline" />Following a geometric transformation, a point might not fall on the grid points in the
new space. This is very likely actually, because the image grid is discrete. So, the
point has a certain starting grey level, and we need to decide where that value
will fall in the discrete grid. This is called interpolation. The easiest way is to put
the point in the nearest neighbour (nearest neighbour grey level interpolation).
<br 
class="newline" />Another way is to share the point to 4 pixels, expressing it as a linear combination.
<br 
class="newline" />So in nearest neighbour grey level interpolation we are just moving grey-levels, but their
values do not change. <br 
class="newline" /><br 
class="newline" />The most common applications of geometric operations are:
      <ul class="itemize1">
      <li class="itemize">
      <!--l. 13--><p class="noindent" >elimination of geometric distortions
      </li>
      <li class="itemize">
      <!--l. 14--><p class="noindent" >scaling the image
      </li>
      <li class="itemize">
      <!--l. 15--><p class="noindent" >rotating the image
                                                                                    
                                                                                    
      </li>
      <li class="itemize">
      <!--l. 16--><p class="noindent" >alignment of images</li></ul>
<!--l. 19--><p class="indent" >   A great tool for studying the distribution of grey levels in an image are image histograms.
<br 
class="newline" />Image histograms show how many times a particular grey-level appears in an image. With
this histograms I can see how much I&#8217;m using the intensity level, but we don&#8217;t
know where those pixels are, so we are completely losing the spatial information. In
addition to this, histograms are not unique: we can have several images all with the
same histogram (you can&#8217;t reconstruct the image starting from the histogram).
<br 
class="newline" />When the constrast is low, the number of grey levels used is low, so the histogram must be
narrow. <br 
class="newline" />On grey-level operation is thresholding, which converts pixel into black or white depending
on whether the original color value is within the threshold range. This is very useful when we
want to discriminate foreground and background.
    
</body></html> 

                                                                                    


